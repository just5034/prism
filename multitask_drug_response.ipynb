{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Multi-Task Drug Response Prediction\n",
    "\n",
    "**Goal**: Predict IC50 values for multiple drugs simultaneously\n",
    "\n",
    "**Why Multi-Task?**\n",
    "- Single-drug prediction can overfit to drug-specific patterns\n",
    "- Multi-task forces the model to learn generalizable biological features\n",
    "- More realistic evaluation: can the model predict drug responses broadly?\n",
    "\n",
    "**Split Strategies**:\n",
    "1. **Random split** (Baseline): Standard 80/20 random split\n",
    "2. **Histology-based split** (Hard): Test on unseen cancer subtypes\n",
    "3. **Site-based split** (Hard): Test on unseen tissue origins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colab-setup-header",
   "metadata": {},
   "source": [
    "## Google Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colab-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "IN_COLAB = 'google.colab' in str(get_ipython()) if 'get_ipython' in dir() else False\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DATA_DIR = '/content/drive/MyDrive/bioai_data'\n",
    "    os.chdir(DATA_DIR)\n",
    "    print(f\"Working directory: {os.getcwd()}\")\n",
    "    print(f\"Files available: {os.listdir('.')}\")\n",
    "else:\n",
    "    print(\"Not running in Colab - using local paths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load multi-omics feature data\n",
    "methexpr_df = pd.read_csv('ml_with_gene_expr.csv.gz', compression='gzip', index_col=0, low_memory=False)\n",
    "\n",
    "metadata_cols = ['primary site', 'primary histology', 'cosmic_id']\n",
    "methylation_cols = [col for col in methexpr_df.columns if col.startswith('cg')]\n",
    "expression_cols = [col for col in methexpr_df.columns if col.startswith('expr_')]\n",
    "\n",
    "print(f\"Feature data: {methexpr_df.shape}\")\n",
    "print(f\"Methylation: {len(methylation_cols)} | Expression: {len(expression_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-drugs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load drug response data\n",
    "response_df = pd.read_csv('ML_dataset_methylation_drug_response.csv.gz', compression='gzip', index_col=0, low_memory=False)\n",
    "\n",
    "drug_cols = [col for col in response_df.columns if not col.startswith('cg') and col not in metadata_cols]\n",
    "print(f\"Drug response data: {response_df.shape}\")\n",
    "print(f\"Total drugs: {len(drug_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drug-selection-header",
   "metadata": {},
   "source": [
    "## Drug Selection by Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drug-coverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze and select drugs with high coverage\n",
    "drug_response = response_df[drug_cols]\n",
    "coverage = drug_response.notna().sum().sort_values(ascending=False)\n",
    "total_samples = len(response_df)\n",
    "\n",
    "COVERAGE_THRESHOLD = 0.90\n",
    "min_samples = int(COVERAGE_THRESHOLD * total_samples)\n",
    "selected_drugs = coverage[coverage >= min_samples].index.tolist()\n",
    "\n",
    "print(f\"Coverage threshold: {COVERAGE_THRESHOLD*100:.0f}% ({min_samples} samples)\")\n",
    "print(f\"Selected drugs: {len(selected_drugs)}\")\n",
    "print(f\"\\nTop 5: {selected_drugs[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-prep-header",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge and filter data\n",
    "df = methexpr_df.join(response_df[selected_drugs], how='inner')\n",
    "\n",
    "# Filter to cells with good drug coverage\n",
    "Y_all = df[selected_drugs]\n",
    "cell_coverage = Y_all.notna().sum(axis=1) / len(selected_drugs)\n",
    "df_filtered = df[cell_coverage >= 0.95].copy()\n",
    "\n",
    "# Prepare X and Y\n",
    "feature_cols = methylation_cols + expression_cols\n",
    "X = df_filtered[feature_cols].dropna(axis=1)  # Drop cols with NaN\n",
    "Y = df_filtered[selected_drugs].fillna(df_filtered[selected_drugs].median())  # Impute Y\n",
    "\n",
    "print(f\"Final dataset:\")\n",
    "print(f\"  X: {X.shape}\")\n",
    "print(f\"  Y: {Y.shape}\")\n",
    "print(f\"  Dropped {len(feature_cols) - X.shape[1]} features with NaN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-header",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multitask(model, X_train, Y_train, X_test, Y_test):\n",
    "    \"\"\"Evaluate multi-output regression model.\"\"\"\n",
    "    Y_pred_train = model.predict(X_train)\n",
    "    Y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    Y_pred_train_df = pd.DataFrame(Y_pred_train, columns=Y_train.columns, index=Y_train.index)\n",
    "    Y_pred_test_df = pd.DataFrame(Y_pred_test, columns=Y_test.columns, index=Y_test.index)\n",
    "    \n",
    "    train_r2, train_mse, test_r2, test_mse = [], [], [], []\n",
    "    for drug in Y_train.columns:\n",
    "        train_r2.append(r2_score(Y_train[drug], Y_pred_train_df[drug]))\n",
    "        train_mse.append(mean_squared_error(Y_train[drug], Y_pred_train_df[drug]))\n",
    "        test_r2.append(r2_score(Y_test[drug], Y_pred_test_df[drug]))\n",
    "        test_mse.append(mean_squared_error(Y_test[drug], Y_pred_test_df[drug]))\n",
    "    \n",
    "    return {\n",
    "        'train_r2': np.mean(train_r2),\n",
    "        'train_mse': np.mean(train_mse),\n",
    "        'test_r2': np.mean(test_r2),\n",
    "        'test_mse': np.mean(test_mse),\n",
    "        'pct_positive': np.mean(np.array(test_r2) > 0) * 100,\n",
    "        'per_drug_r2': dict(zip(Y_train.columns, test_r2)),\n",
    "    }\n",
    "\n",
    "def print_results(results, name):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Train - R2: {results['train_r2']:.4f} | MSE: {results['train_mse']:.4f}\")\n",
    "    print(f\"Test  - R2: {results['test_r2']:.4f} | MSE: {results['test_mse']:.4f}\")\n",
    "    print(f\"Drugs with R2>0: {results['pct_positive']:.1f}%\")\n",
    "\n",
    "def create_benchmark():\n",
    "    return pd.DataFrame(columns=['Model', 'Train R2', 'Train MSE', 'Test R2', 'Test MSE', '% R2>0'])\n",
    "\n",
    "def add_to_benchmark(df, name, results):\n",
    "    df.loc[len(df)] = [name, results['train_r2'], results['train_mse'], \n",
    "                       results['test_r2'], results['test_mse'], results['pct_positive']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_COMPONENTS = 50\n",
    "SPLIT_THRESHOLD = 0.40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "random-header",
   "metadata": {},
   "source": [
    "---\n",
    "# Split 0: Random (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_r, X_test_r, Y_train_r, Y_test_r = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler_r = RobustScaler()\n",
    "X_train_r_s = scaler_r.fit_transform(X_train_r)\n",
    "X_test_r_s = scaler_r.transform(X_test_r)\n",
    "\n",
    "pca_r = PCA(n_components=N_COMPONENTS)\n",
    "X_train_r_pca = pca_r.fit_transform(X_train_r_s)\n",
    "X_test_r_pca = pca_r.transform(X_test_r_s)\n",
    "\n",
    "print(f\"Random Split: Train {len(X_train_r)} | Test {len(X_test_r)}\")\n",
    "print(f\"PCA variance: {pca_r.explained_variance_ratio_.sum()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_r = create_benchmark()\n",
    "\n",
    "# Ridge\n",
    "ridge_r = Ridge(alpha=1.0)\n",
    "ridge_r.fit(X_train_r_pca, Y_train_r)\n",
    "res_ridge_r = evaluate_multitask(ridge_r, X_train_r_pca, Y_train_r, X_test_r_pca, Y_test_r)\n",
    "print_results(res_ridge_r, 'Ridge (Random)')\n",
    "bench_r = add_to_benchmark(bench_r, 'Ridge', res_ridge_r)\n",
    "\n",
    "# Random Forest\n",
    "rf_r = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "rf_r.fit(X_train_r_pca, Y_train_r)\n",
    "res_rf_r = evaluate_multitask(rf_r, X_train_r_pca, Y_train_r, X_test_r_pca, Y_test_r)\n",
    "print_results(res_rf_r, 'Random Forest (Random)')\n",
    "bench_r = add_to_benchmark(bench_r, 'RandomForest', res_rf_r)\n",
    "\n",
    "# XGBoost\n",
    "print(f\"\\nTraining XGBoost ({len(selected_drugs)} drugs)...\")\n",
    "xgb_r = MultiOutputRegressor(XGBRegressor(n_estimators=100, max_depth=5, learning_rate=0.1, \n",
    "                                          random_state=42, tree_method='hist', device='cuda', verbosity=0), n_jobs=1)\n",
    "xgb_r.fit(X_train_r_pca, Y_train_r)\n",
    "res_xgb_r = evaluate_multitask(xgb_r, X_train_r_pca, Y_train_r, X_test_r_pca, Y_test_r)\n",
    "print_results(res_xgb_r, 'XGBoost (Random)')\n",
    "bench_r = add_to_benchmark(bench_r, 'XGBoost', res_xgb_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RANDOM SPLIT (Baseline)\")\n",
    "print(f\"Train: {len(X_train_r)} | Test: {len(X_test_r)}\")\n",
    "print(\"=\"*60)\n",
    "display(bench_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hist-header",
   "metadata": {},
   "source": [
    "---\n",
    "# Split 1: Histology-Based (Unseen Cancer Subtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hist-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "histology = df_filtered['primary histology']\n",
    "hist_counts = histology.value_counts(ascending=True)\n",
    "split_idx = int(len(hist_counts) * SPLIT_THRESHOLD)\n",
    "test_hist = hist_counts.index[:split_idx].tolist()\n",
    "train_hist = hist_counts.index[split_idx:].tolist()\n",
    "\n",
    "X_train_h = X[histology.isin(train_hist)]\n",
    "X_test_h = X[histology.isin(test_hist)]\n",
    "Y_train_h = Y[histology.isin(train_hist)]\n",
    "Y_test_h = Y[histology.isin(test_hist)]\n",
    "\n",
    "scaler_h = RobustScaler()\n",
    "X_train_h_s = scaler_h.fit_transform(X_train_h)\n",
    "X_test_h_s = scaler_h.transform(X_test_h)\n",
    "\n",
    "pca_h = PCA(n_components=N_COMPONENTS)\n",
    "X_train_h_pca = pca_h.fit_transform(X_train_h_s)\n",
    "X_test_h_pca = pca_h.transform(X_test_h_s)\n",
    "\n",
    "print(f\"Histology Split: Train {len(X_train_h)} | Test {len(X_test_h)}\")\n",
    "print(f\"Test histologies: {test_hist[:5]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hist-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_h = create_benchmark()\n",
    "\n",
    "ridge_h = Ridge(alpha=1.0)\n",
    "ridge_h.fit(X_train_h_pca, Y_train_h)\n",
    "res_ridge_h = evaluate_multitask(ridge_h, X_train_h_pca, Y_train_h, X_test_h_pca, Y_test_h)\n",
    "print_results(res_ridge_h, 'Ridge (Histology)')\n",
    "bench_h = add_to_benchmark(bench_h, 'Ridge', res_ridge_h)\n",
    "\n",
    "rf_h = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "rf_h.fit(X_train_h_pca, Y_train_h)\n",
    "res_rf_h = evaluate_multitask(rf_h, X_train_h_pca, Y_train_h, X_test_h_pca, Y_test_h)\n",
    "print_results(res_rf_h, 'Random Forest (Histology)')\n",
    "bench_h = add_to_benchmark(bench_h, 'RandomForest', res_rf_h)\n",
    "\n",
    "print(f\"\\nTraining XGBoost ({len(selected_drugs)} drugs)...\")\n",
    "xgb_h = MultiOutputRegressor(XGBRegressor(n_estimators=100, max_depth=5, learning_rate=0.1,\n",
    "                                          random_state=42, tree_method='hist', device='cuda', verbosity=0), n_jobs=1)\n",
    "xgb_h.fit(X_train_h_pca, Y_train_h)\n",
    "res_xgb_h = evaluate_multitask(xgb_h, X_train_h_pca, Y_train_h, X_test_h_pca, Y_test_h)\n",
    "print_results(res_xgb_h, 'XGBoost (Histology)')\n",
    "bench_h = add_to_benchmark(bench_h, 'XGBoost', res_xgb_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hist-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HISTOLOGY SPLIT (Unseen Cancer Subtypes)\")\n",
    "print(f\"Train: {len(X_train_h)} | Test: {len(X_test_h)}\")\n",
    "print(\"=\"*60)\n",
    "display(bench_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "site-header",
   "metadata": {},
   "source": [
    "---\n",
    "# Split 2: Site-Based (Unseen Tissue Origins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "site-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "site = df_filtered['primary site']\n",
    "site_counts = site.value_counts(ascending=True)\n",
    "site_idx = int(len(site_counts) * SPLIT_THRESHOLD)\n",
    "test_site = site_counts.index[:site_idx].tolist()\n",
    "train_site = site_counts.index[site_idx:].tolist()\n",
    "\n",
    "X_train_s = X[site.isin(train_site)]\n",
    "X_test_s = X[site.isin(test_site)]\n",
    "Y_train_s = Y[site.isin(train_site)]\n",
    "Y_test_s = Y[site.isin(test_site)]\n",
    "\n",
    "scaler_s = RobustScaler()\n",
    "X_train_s_s = scaler_s.fit_transform(X_train_s)\n",
    "X_test_s_s = scaler_s.transform(X_test_s)\n",
    "\n",
    "pca_s = PCA(n_components=N_COMPONENTS)\n",
    "X_train_s_pca = pca_s.fit_transform(X_train_s_s)\n",
    "X_test_s_pca = pca_s.transform(X_test_s_s)\n",
    "\n",
    "print(f\"Site Split: Train {len(X_train_s)} | Test {len(X_test_s)}\")\n",
    "print(f\"Test sites: {test_site}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "site-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_s = create_benchmark()\n",
    "\n",
    "ridge_s = Ridge(alpha=1.0)\n",
    "ridge_s.fit(X_train_s_pca, Y_train_s)\n",
    "res_ridge_s = evaluate_multitask(ridge_s, X_train_s_pca, Y_train_s, X_test_s_pca, Y_test_s)\n",
    "print_results(res_ridge_s, 'Ridge (Site)')\n",
    "bench_s = add_to_benchmark(bench_s, 'Ridge', res_ridge_s)\n",
    "\n",
    "rf_s = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "rf_s.fit(X_train_s_pca, Y_train_s)\n",
    "res_rf_s = evaluate_multitask(rf_s, X_train_s_pca, Y_train_s, X_test_s_pca, Y_test_s)\n",
    "print_results(res_rf_s, 'Random Forest (Site)')\n",
    "bench_s = add_to_benchmark(bench_s, 'RandomForest', res_rf_s)\n",
    "\n",
    "print(f\"\\nTraining XGBoost ({len(selected_drugs)} drugs)...\")\n",
    "xgb_s = MultiOutputRegressor(XGBRegressor(n_estimators=100, max_depth=5, learning_rate=0.1,\n",
    "                                          random_state=42, tree_method='hist', device='cuda', verbosity=0), n_jobs=1)\n",
    "xgb_s.fit(X_train_s_pca, Y_train_s)\n",
    "res_xgb_s = evaluate_multitask(xgb_s, X_train_s_pca, Y_train_s, X_test_s_pca, Y_test_s)\n",
    "print_results(res_xgb_s, 'XGBoost (Site)')\n",
    "bench_s = add_to_benchmark(bench_s, 'XGBoost', res_xgb_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "site-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SITE SPLIT (Unseen Tissue Origins)\")\n",
    "print(f\"Train: {len(X_train_s)} | Test: {len(X_test_s)}\")\n",
    "print(\"=\"*60)\n",
    "display(bench_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison-header",
   "metadata": {},
   "source": [
    "---\n",
    "# Final Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"MULTI-TASK DRUG RESPONSE: FINAL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Dataset: {len(X)} cells x {len(selected_drugs)} drugs\")\n",
    "print(f\"Features: {X.shape[1]} -> {N_COMPONENTS} PCA\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(f\"RANDOM (Baseline) | Train: {len(X_train_r)} | Test: {len(X_test_r)}\")\n",
    "print(\"-\"*70)\n",
    "display(bench_r)\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(f\"HISTOLOGY (Hard) | Train: {len(X_train_h)} | Test: {len(X_test_h)}\")\n",
    "print(\"-\"*70)\n",
    "display(bench_h)\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(f\"SITE (Hard) | Train: {len(X_train_s)} | Test: {len(X_test_s)}\")\n",
    "print(\"-\"*70)\n",
    "display(bench_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpretation",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "1. RANDOM (Baseline): Test similar to train -> higher R2 expected\n",
    "2. HISTOLOGY (Hard): Unseen cancer subtypes -> tests subtype generalization  \n",
    "3. SITE (Hard): Unseen tissues -> tests tissue generalization\n",
    "\n",
    "Key metric: '% R2>0' = fraction of drugs predicted better than mean\n",
    "Negative R2 = model worse than predicting the mean for all samples\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis-header",
   "metadata": {},
   "source": [
    "## Per-Drug Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "per-drug-plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_r2 = pd.Series(res_xgb_r['per_drug_r2'])\n",
    "hist_r2 = pd.Series(res_xgb_h['per_drug_r2'])\n",
    "site_r2 = pd.Series(res_xgb_s['per_drug_r2'])\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].hist(rand_r2, bins=30, alpha=0.7, label='Random', color='green')\n",
    "axes[0].hist(hist_r2, bins=30, alpha=0.7, label='Histology', color='blue')\n",
    "axes[0].hist(site_r2, bins=30, alpha=0.7, label='Site', color='orange')\n",
    "axes[0].axvline(x=0, color='red', linestyle='--')\n",
    "axes[0].set_xlabel('Test R2')\n",
    "axes[0].set_title('Per-Drug R2 Distribution')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].scatter(rand_r2, hist_r2, alpha=0.5, c='blue')\n",
    "axes[1].axhline(0, color='red', linestyle='--', alpha=0.5)\n",
    "axes[1].axvline(0, color='red', linestyle='--', alpha=0.5)\n",
    "axes[1].plot([-0.5, 0.5], [-0.5, 0.5], 'k--', alpha=0.3)\n",
    "axes[1].set_xlabel('Random R2')\n",
    "axes[1].set_ylabel('Histology R2')\n",
    "axes[1].set_title('Random vs Histology')\n",
    "\n",
    "axes[2].scatter(rand_r2, site_r2, alpha=0.5, c='orange')\n",
    "axes[2].axhline(0, color='red', linestyle='--', alpha=0.5)\n",
    "axes[2].axvline(0, color='red', linestyle='--', alpha=0.5)\n",
    "axes[2].plot([-0.5, 0.5], [-0.5, 0.5], 'k--', alpha=0.3)\n",
    "axes[2].set_xlabel('Random R2')\n",
    "axes[2].set_ylabel('Site R2')\n",
    "axes[2].set_title('Random vs Site')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nR2 drop from Random baseline:\")\n",
    "print(f\"  -> Histology: {(rand_r2 - hist_r2).mean():.4f}\")\n",
    "print(f\"  -> Site: {(rand_r2 - site_r2).mean():.4f}\")\n",
    "print(f\"\\nDrugs with R2>0:\")\n",
    "print(f\"  Random: {(rand_r2>0).sum()}/{len(rand_r2)} ({100*(rand_r2>0).mean():.1f}%)\")\n",
    "print(f\"  Histology: {(hist_r2>0).sum()}/{len(hist_r2)} ({100*(hist_r2>0).mean():.1f}%)\")\n",
    "print(f\"  Site: {(site_r2>0).sum()}/{len(site_r2)} ({100*(site_r2>0).mean():.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
